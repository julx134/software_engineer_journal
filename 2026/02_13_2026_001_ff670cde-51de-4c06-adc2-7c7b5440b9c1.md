- Initially designed the deserializer and mapper to instantiate an object for every `adv` field due to 'semantic' data format reasonings

- however, after confirming with my manager, we discussed constraints and the fact that we are going to be serving 400+ devices concurrently will not be a good design for the system because it will be 400+ object instances in memory per 1-2s.

- There was also an extra requirement in caching the device battery information (derived from the mqtt broker) into redis.

- However, the MQTT broker and listeners are already complicated via dynamic configuration and polymorphism + the fact that the SOSListener is also performing business-specific logic on top of mqtt message deserialization and filtering. The design was already too coupled (i.e. MQTT client management, message handling and deserialization, and sos business logic handling), that if we added a caching logic that we would have a coupled 'God' Listener.

- I suggested to leverage Spring Event capability since we were already using it in some parts of the code. We would decouple the mqtt logic (client management, subscription, and deserializing) with the business logic (i.e. caching + sos management). In that way, we have the mqtt service/infra which sole purpose is to do mqtt client management and message deserialization, and we could have the observer be correctly part of the business domain logic space.

- Diving deep --> As I was implementing the code, one of the requirements for caching was to only write new values past the expiry threshold (i.e. we set once, wait for it to expire, and then update value). However, I got to thinking about the cost of having to "reject" each public message within that period which was minimal because it was singular boolean condition check. Then I went further deeper and asked if there was a performance cost in using Spring Events, especially with our expected throughput (2000 msg/sec). From research, it seems that the Spring Bus was light enough to handle message pub and sub. So I went even deeper, and checked to see if our deserialization code could handle the expected throughput. From research, we are far optimized from it because we are using JSON Node Parsing (which is inherently expensive because you have to build a hashmap). I went to profile it and it cost us 297ms in execution time for 400 msgs.

- Sidenote --> We implemented with JSON Node which has to build a tree structure of the mqtt message first. Each primitive field is wrapped with a JSON Node equivalent and each objec is wrapped with a LinkedHashMap. This makes the memory usage huge and also more processing time due to having to initialize the node wrappers and building the tree.

- This means we can't reach our throughput because we can only theoretically calculate 1346 msg/s (attach equations here). With the new design, we had a
  98% (2x) faster throughput with a processing reduction of 50% (insert equations here) at 150ms execution time for 400 msg/s

- extra: researched about async programming and spring proxies
