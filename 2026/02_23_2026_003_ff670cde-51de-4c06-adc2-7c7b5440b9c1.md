# Journal Entry: February 15, 2026

**Project ID:** ff670cde-51de-4c06-adc2-7c7b5440b9c1 <br>
**Series:** 003 <br>
**TAGS:** #high_throughput, #jackson_streaming, #profiling, #garbage_collection, #optimization <br>

> **TLDR:** Pushed to support **4000 msg/s**. Moved from Jackson DOM parsing to `readValue()` bindings (**200ms**), then down to the bare-metal Jackson Streaming API (**110ms**). Discovered that at high throughput, memory management (Stack vs. Heap) and GC cycles dictate performance far more than CPU calculation speed. Finally, realized I was misreading the IntelliJ profiler—our baseline was already fast enough, but the journey was a good learning experience and refresher in JVM internals.

## Keep Optimizing

I went over the Spring Async Event architecture with my manager (he approved) as well as the performance of the deserializer function. I had explained that we were still able to optimize by manually mapping fields and not having to rely on Jackson for automatic mapping. He was concerned that our deserializer function was not enough for our expected through. Looking back, my perspective was that the 361ms benchmark wasn't going to cut it; the deserialization function was still eating 18% of the CPU flame graph. I had to rethink how we were parsing data at a fundamental level.

## Deep Dive: The Jackson Parsing Spectrum

Through my research in optimizing JSON processing in Java, I was able to learn and implement three distinct types of deserializing approaches

### Approach 1: DOM Parsing (`readTree`)

This is what I started with. It's the most developer-friendly but the most expensive. Jackson reads the entire byte stream and constructs a massive, hierarchical Document Object Model (DOM) in memory (`JsonNode`). Every single string, integer, and array is wrapped in a heavy Java object (`LinkedHashMap`, `TextNode`, etc.) before you can even touch the data. The memory footprint is big because of the object wrappers and it was not performant because it had process the DOM Tree.

### Approach 2: Token Binding (`readValue`)

My first aggressive optimization was ditching the DOM entirely for the `readValue()` function.
Instead of building a tree, Jackson parses the raw byte stream directly into a sequence of tokens (e.g., `START_OBJECT`, `FIELD_NAME`, `VALUE_STRING`). It then uses **Java Reflection** to dynamically match these tokens to the fields in your annotated POJOs on the fly.

Because our payload is polymorphic (different types of `adv` records), I had to heavily utilize Jackson annotations to guide the token binder.

```java
// EFFICIENT: Let Jackson handle polymorphism at the token-binding level
@JsonTypeInfo(
        use = JsonTypeInfo.Id.NAME,
        include = JsonTypeInfo.As.EXISTING_PROPERTY,
        property = "type",
        visible = true,
        defaultImpl = Void.class
)
@JsonSubTypes({
        @JsonSubTypes.Type(value = DataPayloadA.class, name = "type_a"),
        @JsonSubTypes.Type(value = DataPayloadB.class, name = "type_b")
})
public interface AdvPayload {
    String type();
}

@JsonNaming(PropertyNamingStrategies.SnakeCaseStrategy.class)
public record DataPayloadA(
        String type,
        String uuid,
        int major,
        int minor,
        int rssiAtXm,
        int rssi,
        String tm,
        String mac
) implements AdvPayload {}

@Data
public class RawMqttMessageWrapper {
    private String tm;
    private String gw;
    private Long seq;
    private List<AdvPayload> adv;
}

// Parses the raw byte array directly from Eclipse Paho
RawMqttMessageWrapper rawMsg = rawMessageReader.readValue(msg.getPayload());
...
// Group the payloads by their resolved concrete type
        for (AdvPayload payload : rawMsg.getAdv()) {
            if (payload instanceof DataPayloadA concreteA) {
                groupAList.add(concreteA);
            } else if (payload instanceof DataPayloadB concreteB) {
                groupBList.add(concreteB);
            }
        }

```

**The Result:** Execution time dropped to **200ms** (12.46% of the flame graph). A massive win, but Reflection still has runtime overhead, and I wanted to get the execution under 5%.

### Approach 3: Bare-Metal Jackson Streaming API

To squeeze out maximum performance, I bypassed reflection entirely and used the Jackson Streaming API.

Here, you manually control the parser and iterate through the tokens yourself. I initially avoided this because it's notoriously brittle—if the JSON structure changes unexpectedly, your manual traversal logic breaks. However, I decided to proceed because I confirmed with my manager that we have a strict JSON structure. I made two primary assumptions: first, we would always have a known set of fields, and second, that although we know the expected field order, I shouldn't rely on it. Since we are dealing with live IoT devices sending high throughput over the network, fields can arrive jumbled, and I wanted the code to be resilient.

My idea was to create a buffer for all the fields (or instantiate the variables) as we pass through an entire object. We would find the tokens and work around the known field structure. For primitive fields, we check if the field name is part of our known set and store it. When we reach the `adv` array, we iterate through each entry, buffering the primitive fields within those nested objects and creating the payload objects accordingly.

```java
// BARE METAL: Manually traversing tokens. Extremely fast, very ugly.
while (parser.nextToken() != JsonToken.END_OBJECT) {
    String fieldName = parser.getCurrentName();
    parser.nextToken(); // move to the value

    // Storing primitives locally on the STACK instead of the HEAP
    if (MQTTFieldConstant.TM.equals(fieldName)) {
        tm = parser.getText();
    } else if (MQTTFieldConstant.GW.equals(fieldName)) {
        gw = parser.getText();
    }
    // ... massive switch statement for inner arrays
    } else if (MQTTFieldConstant.ADV.equals(fieldName)) {

        // If adv is null or not an array, break out to return the default wrapper
        if (parser.currentToken() != JsonToken.START_ARRAY) {
            break;
        }

        // Iterate through the array
        while (parser.nextToken() != JsonToken.END_ARRAY) {
            if (parser.currentToken() == JsonToken.START_OBJECT) {

                // Local variables to hold data until we know the type
                String type = null, uuid = null, mac = null, advTm = null, name = null;
                int major = 0, minor = 0, rssiAtXm = 0, rssi = 0, battery = 0;

                // Read the inner object
                while (parser.nextToken() != JsonToken.END_OBJECT) {
                    String advFieldName = parser.getCurrentName();
                    parser.nextToken(); // move to value

                    switch (advFieldName) {
                        case MQTTFieldConstant.TYPE -> type = parser.getText();
}
```

**The Result:** Execution time was **110ms** (6.55% of the flame graph).

## Deep Dive: Stack vs. Heap & The Garbage Collection Tax

I was shocked by how fast the Streaming API was. I even tested **DSL-JSON** (widely considered the fastest Java JSON library because it uses compile-time annotation processing instead of runtime reflection). DSL-JSON clocked in at **142ms** (8%). _Why did my manual Jackson implementation beat a compiled library?_

![Diagram: Line chart comparing Execution Time vs. CPU Utilization Rate for DOM Parsing, readValue Binding, and Streaming API at 400, 4000, and 10000 msg/s](./images/mqtt_optimization_graph.png)

This led to my biggest realization of the project: **At speeds of 4000+ msgs/sec, raw CPU logic computation matters far less than Memory Churn.**

Libraries like DSL-JSON still have to instantiate intermediary Data Transfer Objects (DTOs) on the **Heap**. Every time an object is placed on the heap, the Garbage Collector (GC) eventually has to wake up, pause execution, and clean it up. At high throughput, this creates massive "GC Churn" that eats up CPU cycles.

My manual Jackson Streaming implementation circumvented this. As the parser iterated through the tokens, I buffered the primitive values (like `tm`, `gw`, `seq`) into local method variables. **Local variables live on the Stack.** The Stack is blazingly fast and, more importantly, pops immediately when the method scope ends. There is absolutely zero Garbage Collection overhead for stack allocations.

## The Ultimate Reality Check: I Misread the Profiler

After all this hardcore, bare-metal optimization, I realized something embarrassing. I thought the "6.55%" in IntelliJ meant total CPU utilization. It doesn't.

IntelliJ's flame graph shows the percentage of **_active thread time_**. My Spring Boot server was highly optimized to sleep while waiting for network requests. That 6.55% simply meant that out of the total time the CPU was _actually awake and doing work_ during my 60-second test, deserialization only took 6.5%.

My baseline `readValue()` token-binding approach could have easily handled tens of thousands of messages per second.

## Summary / Key Takeaway

I had essentially over-optimized. However, I have zero regrets. The deep dive into DOM vs. Tokenizing, Stack vs. Heap allocation, and GC churn was a fun learning experience and refresher in JVM internals.

I also improved the system's overall architecture by decoupling the MQTT listener from the business logic and moving to an event-driven architecture. I learned not to make assumptions too quickly.

Even though the performance is now beyond what we strictly need, I still believe this is the best approach—especially regarding garbage collector efficiency. Any approach that prioritizes readability by creating intermediary objects would increase the GC overhead. Moving forward, I’ve learned that in real-time systems, every single line of code matters. As I was researching, I came across a sentence that stuck with me:

> _Designing for high throughput almost always forces you to write code that caters to how the machine wants to read data, rather than how a human wants to read the code._
